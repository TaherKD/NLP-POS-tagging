{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to accomplish the following in this assignment:\n",
    "\n",
    "1. Write the vanilla Viterbi algorithm for assigning POS tags (i.e. without dealing with unknown words) \n",
    "2. Solve the problem of unknown words using at least two techniques. These techniques can use any of the approaches discussed in the class - lexicon, rule-based, probabilistic etc. Note that to implement these techniques, you can either write separate functions and call them from the main Viterbi algorithm, or modify the Viterbi algorithm, or both.\n",
    "3. Compare the tagging accuracy after making these modifications with the vanilla Viterbi algorithm.\n",
    "4. List down at least three cases from the sample test file (i.e. unknown word-tag pairs) which were incorrectly tagged by the original Viterbi POS tagger and got corrected after your modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download treebank and universal_tagset from nltk if not downloaded already. Use - `nltk.download('treebank')` and `nltk.download('universal_tagset')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('61', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('will', 'VERB'),\n",
       "  ('join', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('board', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('Nov.', 'NOUN'),\n",
       "  ('29', 'NUM'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Elsevier', 'NOUN'),\n",
       "  ('N.V.', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Dutch', 'NOUN'),\n",
       "  ('publishing', 'VERB'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('Rudolph', 'NOUN'),\n",
       "  ('Agnew', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('55', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('former', 'ADJ'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Consolidated', 'NOUN'),\n",
       "  ('Gold', 'NOUN'),\n",
       "  ('Fields', 'NOUN'),\n",
       "  ('PLC', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('was', 'VERB'),\n",
       "  ('named', 'VERB'),\n",
       "  ('*-1', 'X'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('this', 'DET'),\n",
       "  ('British', 'ADJ'),\n",
       "  ('industrial', 'ADJ'),\n",
       "  ('conglomerate', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('A', 'DET'),\n",
       "  ('form', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('asbestos', 'NOUN'),\n",
       "  ('once', 'ADV'),\n",
       "  ('used', 'VERB'),\n",
       "  ('*', 'X'),\n",
       "  ('*', 'X'),\n",
       "  ('to', 'PRT'),\n",
       "  ('make', 'VERB'),\n",
       "  ('Kent', 'NOUN'),\n",
       "  ('cigarette', 'NOUN'),\n",
       "  ('filters', 'NOUN'),\n",
       "  ('has', 'VERB'),\n",
       "  ('caused', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('high', 'ADJ'),\n",
       "  ('percentage', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('cancer', 'NOUN'),\n",
       "  ('deaths', 'NOUN'),\n",
       "  ('among', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('workers', 'NOUN'),\n",
       "  ('exposed', 'VERB'),\n",
       "  ('*', 'X'),\n",
       "  ('to', 'PRT'),\n",
       "  ('it', 'PRON'),\n",
       "  ('more', 'ADV'),\n",
       "  ('than', 'ADP'),\n",
       "  ('30', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('ago', 'ADP'),\n",
       "  (',', '.'),\n",
       "  ('researchers', 'NOUN'),\n",
       "  ('reported', 'VERB'),\n",
       "  ('0', 'X'),\n",
       "  ('*T*-1', 'X'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DET'),\n",
       "  ('asbestos', 'NOUN'),\n",
       "  ('fiber', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('crocidolite', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('is', 'VERB'),\n",
       "  ('unusually', 'ADV'),\n",
       "  ('resilient', 'ADJ'),\n",
       "  ('once', 'ADP'),\n",
       "  ('it', 'PRON'),\n",
       "  ('enters', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('lungs', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('with', 'ADP'),\n",
       "  ('even', 'ADV'),\n",
       "  ('brief', 'ADJ'),\n",
       "  ('exposures', 'NOUN'),\n",
       "  ('to', 'PRT'),\n",
       "  ('it', 'PRON'),\n",
       "  ('causing', 'VERB'),\n",
       "  ('symptoms', 'NOUN'),\n",
       "  ('that', 'DET'),\n",
       "  ('*T*-1', 'X'),\n",
       "  ('show', 'VERB'),\n",
       "  ('up', 'PRT'),\n",
       "  ('decades', 'NOUN'),\n",
       "  ('later', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('researchers', 'NOUN'),\n",
       "  ('said', 'VERB'),\n",
       "  ('0', 'X'),\n",
       "  ('*T*-2', 'X'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets have a look at the first 5 sentences\n",
    "nltk_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3718\n",
      "Validation set size: 196\n"
     ]
    }
   ],
   "source": [
    "# create a train and validation split of 95:5 for evaluation purpose\n",
    "random.seed(100)\n",
    "train_set, val_set = train_test_split(nltk_data,test_size=0.05,random_state=100)\n",
    "\n",
    "print('Train set size: '+str(len(train_set)))\n",
    "print('Validation set size: '+str(len(val_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('One', 'NUM'), ('bright', 'ADJ'), ('sign', 'NOUN'), ('is', 'VERB'), ('that', 'ADP'), ('a', 'DET'), ('growing', 'VERB'), ('number', 'NOUN'), ('of', 'ADP'), ('women', 'NOUN'), ('have', 'VERB'), ('entered', 'VERB'), ('the', 'DET'), ('once', 'ADV'), ('male-dominated', 'ADJ'), ('field', 'NOUN'), (';', '.'), ('more', 'ADJ'), ('than', 'ADP'), ('a', 'DET'), ('third', 'ADJ'), ('of', 'ADP'), ('the', 'DET'), ('ringers', 'NOUN'), ('today', 'NOUN'), ('are', 'VERB'), ('women', 'NOUN'), ('.', '.')], [('``', '.'), ('These', 'DET'), ('days', 'NOUN'), (',', '.'), ('banking', 'NOUN'), ('customers', 'NOUN'), ('walk', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('door', 'NOUN'), ('*-1', 'X'), ('expecting', 'VERB'), ('you', 'PRON'), ('to', 'PRT'), ('have', 'VERB'), ('a', 'DET'), ('package', 'NOUN'), ('especially', 'ADV'), ('for', 'ADP'), ('them', 'PRON'), (',', '.'), (\"''\", '.'), ('Ms.', 'NOUN'), ('Moore', 'NOUN'), ('says', 'VERB'), ('*T*-2', 'X'), ('.', '.')], [('The', 'DET'), ('rights', 'NOUN'), (',', '.'), ('which', 'DET'), ('*T*-173', 'X'), ('expire', 'VERB'), ('Nov.', 'NOUN'), ('21', 'NUM'), (',', '.'), ('can', 'VERB'), ('be', 'VERB'), ('exercised', 'VERB'), ('*-104', 'X'), ('for', 'ADP'), ('$', '.'), ('100', 'NUM'), ('*U*', 'X'), ('each', 'DET'), ('.', '.')], [('@', 'ADP')], [('Mr.', 'NOUN'), ('Trudeau', 'NOUN'), (\"'s\", 'PRT'), ('attorney', 'NOUN'), (',', '.'), ('Norman', 'NOUN'), ('K.', 'NOUN'), ('Samnick', 'NOUN'), (',', '.'), ('said', 'VERB'), ('0', 'X'), ('the', 'DET'), ('harassment', 'NOUN'), ('consists', 'VERB'), ('mainly', 'ADV'), ('of', 'ADP'), ('the', 'DET'), ('guild', 'NOUN'), (\"'s\", 'PRT'), ('year-long', 'ADJ'), ('threats', 'NOUN'), ('of', 'ADP'), ('disciplinary', 'ADJ'), ('action', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# have a look at first 5 train sentences\n",
    "print(train_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of tagged words: 95949\n"
     ]
    }
   ],
   "source": [
    "# getting list of tagged words i.e converting from sentence to words format\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "print('Total no. of tagged words: '+str(len(train_tagged_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: ['One', 'bright', 'sign', 'is', 'that', 'a', 'growing', 'number', 'of', 'women']\n",
      "10 most common tokens: [(',', 4650), ('the', 3855), ('.', 3641), ('of', 2216), ('to', 2050), ('a', 1798), ('in', 1501), ('and', 1446), ('*-1', 1064), ('0', 1044)]\n"
     ]
    }
   ],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "print('Sample tokens: '+str(tokens[:10]))\n",
    "print('10 most common tokens: '+str(Counter(tokens).most_common(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12106\n"
     ]
    }
   ],
   "source": [
    "# vocabulary - unique tokens\n",
    "V = set(tokens)\n",
    "print('Vocabulary size: '+str(len(V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tags: 12\n",
      "Tags: ['X', 'VERB', 'PRT', 'PRON', 'NUM', 'NOUN', 'DET', 'CONJ', 'ADV', 'ADP', 'ADJ', '.']\n",
      "10 most common tags: [('NOUN', 27539), ('VERB', 12919), ('.', 11149), ('ADP', 9433), ('DET', 8318), ('X', 6301), ('ADJ', 6105), ('NUM', 3366), ('PRT', 3048), ('ADV', 3004)]\n"
     ]
    }
   ],
   "source": [
    "# number of tags\n",
    "tags = [pair[1] for pair in train_tagged_words]\n",
    "T = sorted(set(tags),reverse=True)\n",
    "print('No. of tags: '+str(len(T)))\n",
    "print('Tags: '+str(T))\n",
    "tags_counter = Counter(tags)\n",
    "print('10 most common tags: '+str(tags_counter.most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing P(w/t) and storing in V x T matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = len(T)\n",
    "v = len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "tag_dict = {}\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    global tag_dict\n",
    "    if tag in tag_dict:\n",
    "        tag_list = tag_dict[tag]\n",
    "    else:\n",
    "        tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "        tag_dict[tag] = tag_list\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# creating v x t emission matrix of tags\n",
    "# each column is t, each row is w\n",
    "# thus M(i, j) represents P(wj given ti)\n",
    "\n",
    "words_tags_matrix = np.zeros((len(V), len(T)), dtype='float32')\n",
    "for i, w in enumerate(list(V)):\n",
    "    for j, t in enumerate(list(T)): \n",
    "        w_g_t = word_given_tag(w, t)\n",
    "        words_tags_matrix[i, j] = w_g_t[0]/w_g_t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aslacton</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Form</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>print</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advise</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assumption</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              X      VERB  PRT  PRON  NUM      NOUN  DET  CONJ  ADV  ADP  ADJ  \\\n",
       "Aslacton    0.0  0.000000  0.0   0.0  0.0  0.000073  0.0   0.0  0.0  0.0  0.0   \n",
       "Form        0.0  0.000000  0.0   0.0  0.0  0.000036  0.0   0.0  0.0  0.0  0.0   \n",
       "print       0.0  0.000542  0.0   0.0  0.0  0.000036  0.0   0.0  0.0  0.0  0.0   \n",
       "advise      0.0  0.000077  0.0   0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0   \n",
       "assumption  0.0  0.000000  0.0   0.0  0.0  0.000109  0.0   0.0  0.0  0.0  0.0   \n",
       "\n",
       "              .  \n",
       "Aslacton    0.0  \n",
       "Form        0.0  \n",
       "print       0.0  \n",
       "advise      0.0  \n",
       "assumption  0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "words_tags_df = pd.DataFrame(words_tags_matrix, columns=list(T), index=list(V))\n",
    "words_tags_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing P(t/t) and storing in T x T matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):    \n",
    "    global tags_counter\n",
    "    count_t1 = tags_counter[t1]\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        t2_g_t1 = t2_given_t1(t2, t1)\n",
    "        tags_matrix[i, j] = t2_g_t1[0]/t2_g_t1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.074433</td>\n",
       "      <td>0.204571</td>\n",
       "      <td>0.184891</td>\n",
       "      <td>0.055705</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.062371</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>0.010316</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.144898</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.162831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.218438</td>\n",
       "      <td>0.168744</td>\n",
       "      <td>0.031427</td>\n",
       "      <td>0.035916</td>\n",
       "      <td>0.022448</td>\n",
       "      <td>0.111386</td>\n",
       "      <td>0.133292</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.082050</td>\n",
       "      <td>0.091184</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.034291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.405184</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.017717</td>\n",
       "      <td>0.056102</td>\n",
       "      <td>0.245735</td>\n",
       "      <td>0.101050</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.083661</td>\n",
       "      <td>0.043635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.092720</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.211494</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.023372</td>\n",
       "      <td>0.072031</td>\n",
       "      <td>0.040613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.211824</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.184195</td>\n",
       "      <td>0.352347</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.035056</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.118835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             X      VERB       PRT      PRON       NUM      NOUN       DET  \\\n",
       "X     0.074433  0.204571  0.184891  0.055705  0.002857  0.062371  0.055229   \n",
       "VERB  0.218438  0.168744  0.031427  0.035916  0.022448  0.111386  0.133292   \n",
       "PRT   0.013123  0.405184  0.001969  0.017717  0.056102  0.245735  0.101050   \n",
       "PRON  0.092720  0.484291  0.012261  0.007663  0.007280  0.211494  0.009195   \n",
       "NUM   0.211824  0.016934  0.026144  0.001485  0.184195  0.352347  0.003862   \n",
       "\n",
       "          CONJ       ADV       ADP       ADJ         .  \n",
       "X     0.010316  0.025393  0.144898  0.016505  0.162831  \n",
       "VERB  0.005186  0.082050  0.091184  0.065640  0.034291  \n",
       "PRT   0.002297  0.010171  0.019357  0.083661  0.043635  \n",
       "PRON  0.004981  0.034100  0.023372  0.072031  0.040613  \n",
       "NUM   0.013072  0.002674  0.035056  0.033571  0.118835  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))\n",
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veterbi heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            #w_g_t = word_given_tag(words[key], tag)\n",
    "            #emission_p = w_g_t[0]/w_g_t[1]\n",
    "            emission_p = 0\n",
    "            if word in words_tags_df.index:\n",
    "                emission_p = words_tags_df.loc[word,tag]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of untagged words from validation set\n",
    "val_tagged_words = [tup[0] for sent in val_set for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tagging the validation sentences\n",
    "val_tagged_seq = Viterbi(val_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.905013750793315\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "val_run_base = [tup for sent in val_set for tup in sent]\n",
    "check = [i for i, j in zip(val_tagged_seq, val_run_base) if i == j]\n",
    "accuracy = len(check)/len(val_tagged_seq)\n",
    "print('Accuracy on validation set: '+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing y_valid and y_pred for later comparision\n",
    "labels = sorted(T)\n",
    "y_valid = [tup[1] for tup in val_run_base]\n",
    "y_pred = [tup[1] for tup in val_tagged_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at first 10 incorrectly tagged cases from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('to', 'PRT'), (('book', 'NOUN'), ('book', 'VERB'))],\n",
       " [('leaving', 'VERB'), (('stocks', 'NOUN'), ('stocks', 'ADV'))],\n",
       " [('stocks', 'ADV'), (('up', 'PRT'), ('up', 'ADP'))],\n",
       " [('carried', 'VERB'), (('over', 'ADP'), ('over', 'PRT'))],\n",
       " [('apparently', 'ADV'), (('ignored', 'X'), ('ignored', 'VERB'))],\n",
       " [('ignored', 'VERB'), (('reports', 'VERB'), ('reports', 'NOUN'))],\n",
       " [('Chilean', 'ADJ'), (('mine', 'NOUN'), ('mine', 'ADJ'))],\n",
       " [('the', 'DET'), (('Palestinian', 'ADJ'), ('Palestinian', 'NOUN'))],\n",
       " [('committee', 'NOUN'), (('first', 'ADJ'), ('first', 'ADV'))],\n",
       " [('.', '.'), (('Preston', 'X'), ('Preston', 'NOUN'))]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases = [[val_run_base[i-1],j] for i, j in enumerate(zip(val_tagged_seq, val_run_base)) if j[0]!=j[1]]\n",
    "incorrect_tagged_cases[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all unknown words are marked as X because word's emmision probability is zero. Lets solve the problem at hand using smoothing, lexicon, rule and probability based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. One-Count Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the Appendix of <a href='http://www.cs.jhu.edu/~jason/465/hw-hmm/hw-hmm.pdf'>Jason Eisner's HMM tutorial</a>, the basic idea here is that for unknown words more probability mass should be given to tags that appear with a wider variety of low frequency words. For example, since the tag NOUN appears on a large number of different words and DETERMINER appears on a small number of different words, it is more likely that an unseen word will be a NOUN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest frequency tag: [('NOUN', 27539)]\n"
     ]
    }
   ],
   "source": [
    "# find the most widely spread tag\n",
    "print('Highest frequency tag: '+str(tags_counter.most_common(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'NUM': 926, 'ADJ': 1772, 'NOUN': 6402, 'VERB': 2566, 'ADP': 118, 'DET': 46, 'ADV': 457, '.': 22, 'X': 443, 'PRON': 43, 'PRT': 21, 'CONJ': 18})\n"
     ]
    }
   ],
   "source": [
    "# lets find the highest frequency tag considering unique words\n",
    "count_dict = defaultdict(set)\n",
    "for tup in train_tagged_words:\n",
    "    count_dict[tup[1]].add(tup[0])\n",
    "for c in count_dict:\n",
    "    count_dict[c] = len(count_dict[c])\n",
    "print(count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after considering only unique words, NOUN appears most number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Viterbi Heuristic\n",
    "def Modified_Viterbi_1(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            #w_g_t = word_given_tag(words[key], tag)\n",
    "            #emission_p = w_g_t[0]/w_g_t[1]\n",
    "            emission_p = 0\n",
    "            if word in words_tags_df.index:\n",
    "                emission_p = words_tags_df.loc[word,tag]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # handling unknown words\n",
    "        if pmax == 0:\n",
    "            state.append(tags_counter.most_common(1)[0][0])\n",
    "        else:\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)] \n",
    "            state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy of modified viterbi 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tagging the validation sentences\n",
    "val_tagged_seq = Modified_Viterbi_1(val_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.9348423947535435\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "val_run_base = [tup for sent in val_set for tup in sent]\n",
    "check = [i for i, j in zip(val_tagged_seq, val_run_base) if i == j]\n",
    "accuracy = len(check)/len(val_tagged_seq)\n",
    "print('Accuracy on validation set: '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The accuracy has increased considerably (~3.5%) as compared to vanilla veterbi algorithm. This algorithm uses a better fallback estimation for unknown words.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing y_valid and y_pred for later comparision\n",
    "y_valid_1 = [tup[1] for tup in val_run_base]\n",
    "y_pred_1 = [tup[1] for tup in val_tagged_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lexicon and Rule based solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Bigram + Unigram + Regex Tagger as a fallback tagger of vanilla viterbi algorithm to produce better estimation for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify patterns for tagging\n",
    "patterns = [\n",
    "    (r'.*ly$', 'ADJ'),              # adjective\n",
    "    (r'.*ful$', 'ADJ'),              # adjective\n",
    "    (r'.*ish$', 'ADJ'),              # adjective\n",
    "    (r'.*less$', 'ADJ'),              # adjective\n",
    "    (r'.*st$', 'NUM'),              # number such as 1st\n",
    "    (r'.*rd$', 'NUM'),              # number such as 3rd\n",
    "    (r'.*th$', 'NUM'),              # number such as 11th\n",
    "    (r'.*ed$', 'VERB'),               # past tense\n",
    "    (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*es$', 'VERB'),               # 3rd singular present\n",
    "    (r'.*ould$', 'VERB'),              # modals\n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "    (r'.*', 'NOUN')                    # nouns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "# lexicon backed up by the rule-based tagger\n",
    "unigram_tagger = nltk.UnigramTagger(train_set, backoff=rule_based_tagger)\n",
    "\n",
    "bigram_tagger = nltk.BigramTagger(train_set, backoff=unigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Modified_Viterbi_2(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            #w_g_t = word_given_tag(words[key], tag)\n",
    "            #emission_p = w_g_t[0]/w_g_t[1]\n",
    "            emission_p = 0\n",
    "            if word in words_tags_df.index:\n",
    "                emission_p = words_tags_df.loc[word,tag]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # handling unknown words using backoff tagger\n",
    "        if pmax == 0:\n",
    "            state.append(bigram_tagger.tag([word])[0][1])\n",
    "        else:\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)] \n",
    "            state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy of modified viterbi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tagging the validation sentences\n",
    "val_tagged_seq = Modified_Viterbi_2(val_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.945631478739158\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "val_run_base = [tup for sent in val_set for tup in sent]\n",
    "check = [i for i, j in zip(val_tagged_seq, val_run_base) if i == j]\n",
    "accuracy = len(check)/len(val_tagged_seq)\n",
    "print('Accuracy on validation set: '+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The accuracy has increased considerably (~4.5%) as compared to vanilla veterbi algorithm as well as modified viterbi 1. This algorithm uses a better fallback estimation for unknown words.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing y_valid and y_pred for later comparision\n",
    "y_valid_2 = [tup[1] for tup in val_run_base]\n",
    "y_pred_2 = [tup[1] for tup in val_tagged_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Viterbi Algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .      1.000     1.000     1.000       566\n",
      "         ADJ      0.874     0.740     0.801       292\n",
      "         ADP      0.941     0.979     0.960       424\n",
      "         ADV      0.919     0.820     0.867       167\n",
      "        CONJ      1.000     1.000     1.000       108\n",
      "         DET      0.982     0.956     0.969       407\n",
      "        NOUN      0.969     0.852     0.907      1328\n",
      "         NUM      0.993     0.806     0.890       180\n",
      "        PRON      0.984     1.000     0.992       127\n",
      "         PRT      0.943     0.971     0.957       171\n",
      "        VERB      0.954     0.876     0.914       645\n",
      "           X      0.512     1.000     0.678       312\n",
      "\n",
      "    accuracy                          0.905      4727\n",
      "   macro avg      0.923     0.917     0.911      4727\n",
      "weighted avg      0.933     0.905     0.912      4727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class-wise scores\n",
    "print(classification_report(y_valid, y_pred, labels=labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi algorithm with one-count smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1-score of `NOUN` as well as `X` tags have increased considerably due to better backoff estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .      1.000     1.000     1.000       566\n",
      "         ADJ      0.878     0.736     0.801       292\n",
      "         ADP      0.937     0.979     0.957       424\n",
      "         ADV      0.919     0.820     0.867       167\n",
      "        CONJ      1.000     1.000     1.000       108\n",
      "         DET      0.982     0.956     0.969       407\n",
      "        NOUN      0.869     0.968     0.916      1328\n",
      "         NUM      0.993     0.806     0.890       180\n",
      "        PRON      0.984     1.000     0.992       127\n",
      "         PRT      0.949     0.971     0.960       171\n",
      "        VERB      0.958     0.876     0.915       645\n",
      "           X      1.000     0.962     0.980       312\n",
      "\n",
      "    accuracy                          0.935      4727\n",
      "   macro avg      0.956     0.923     0.937      4727\n",
      "weighted avg      0.937     0.935     0.934      4727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class-wise scores\n",
    "print(classification_report(y_valid_1, y_pred_1, labels=labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm with lexicon and rule based tagger backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1-score for `NOUN`, `VERB` and `NUM` has increased considerably due to regex and probabilistic backoff methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .      1.000     1.000     1.000       566\n",
      "         ADJ      0.863     0.736     0.795       292\n",
      "         ADP      0.937     0.979     0.957       424\n",
      "         ADV      0.913     0.820     0.864       167\n",
      "        CONJ      1.000     1.000     1.000       108\n",
      "         DET      0.982     0.956     0.969       407\n",
      "        NOUN      0.922     0.956     0.939      1328\n",
      "         NUM      0.941     0.978     0.959       180\n",
      "        PRON      0.984     1.000     0.992       127\n",
      "         PRT      0.948     0.965     0.957       171\n",
      "        VERB      0.929     0.933     0.931       645\n",
      "           X      1.000     0.962     0.980       312\n",
      "\n",
      "    accuracy                          0.946      4727\n",
      "   macro avg      0.952     0.940     0.945      4727\n",
      "weighted avg      0.945     0.946     0.945      4727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class-wise scores\n",
    "print(classification_report(y_valid_2, y_pred_2, labels=labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Android', 'is', 'a', 'mobile', 'operating', 'system', 'developed', 'by', 'Google', '.', 'Android', 'has', 'been', 'the', 'best-selling', 'OS', 'worldwide', 'on', 'smartphones', 'since', '2011', 'and', 'on', 'tablets', 'since', '2013', '.', 'Google', 'and', 'Twitter', 'made', 'a', 'deal', 'in', '2015', 'that', 'gave', 'Google', 'access', 'to', 'Twitter', \"'s\", 'firehose', '.', 'Twitter', 'is', 'an', 'online', 'news', 'and', 'social', 'networking', 'service', 'on', 'which', 'users', 'post', 'and', 'interact', 'with', 'messages', 'known', 'as', 'tweets', '.', 'Before', 'entering', 'politics', ',', 'Donald', 'Trump', 'was', 'a', 'domineering', 'businessman', 'and', 'a', 'television', 'personality', '.', 'The', '2018', 'FIFA', 'World', 'Cup', 'is', 'the', '21st', 'FIFA', 'World', 'Cup', ',', 'an', 'international', 'football', 'tournament', 'contested', 'once', 'every', 'four', 'years', '.', 'This', 'is', 'the', 'first', 'World', 'Cup', 'to', 'be', 'held', 'in', 'Eastern', 'Europe', 'and', 'the', '11th', 'time', 'that', 'it', 'has', 'been', 'held', 'in', 'Europe', '.', 'Show', 'me', 'the', 'cheapest', 'round', 'trips', 'from', 'Dallas', 'to', 'Atlanta', 'I', 'would', 'like', 'to', 'see', 'flights', 'from', 'Denver', 'to', 'Philadelphia', '.', 'Show', 'me', 'the', 'price', 'of', 'the', 'flights', 'leaving', 'Atlanta', 'at', 'about', '3', 'in', 'the', 'afternoon', 'and', 'arriving', 'in', 'San', 'Francisco', '.', 'NASA', 'invited', 'social', 'media', 'users', 'to', 'experience', 'the', 'launch', 'of', 'ICESAT-2', 'Satellite', '.']\n"
     ]
    }
   ],
   "source": [
    "test_set = []\n",
    "with open('Test_sentences.txt') as test_file:\n",
    "    for line in test_file:\n",
    "        test_set.extend(word_tokenize(line))\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'X'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'X'), ('.', '.'), ('Android', 'X'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'X'), ('worldwide', 'X'), ('on', 'ADP'), ('smartphones', 'X'), ('since', 'ADP'), ('2011', 'X'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'X'), ('.', '.'), ('Google', 'X'), ('and', 'CONJ'), ('Twitter', 'X'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'X'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'X'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'X'), (\"'s\", 'PRT'), ('firehose', 'X'), ('.', '.'), ('Twitter', 'X'), ('is', 'VERB'), ('an', 'DET'), ('online', 'X'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'X'), ('with', 'ADP'), ('messages', 'X'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'X'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'X'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'X'), ('.', '.'), ('The', 'DET'), ('2018', 'X'), ('FIFA', 'X'), ('World', 'NOUN'), ('Cup', 'X'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'X'), ('FIFA', 'X'), ('World', 'NOUN'), ('Cup', 'X'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'X'), ('contested', 'X'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'X'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'X'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'X'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'X'), ('invited', 'X'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'X'), ('Satellite', 'X'), ('.', '.')]\n",
      "Wall time: 86.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tagging the test sentences\n",
    "test_tagged_seq = Viterbi(test_set)\n",
    "print(test_tagged_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Following are the highlighted tags which are corrected in subsequent methods:***\n",
    "1. `('Android', 'X')`, ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), `('Google', 'X')`, ('.', '.'), `('Android', 'X')`, ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), `('OS', 'X')`, `('worldwide', 'X')`, ('on', 'ADP'), `('smartphones', 'X')`, ('since', 'ADP'), `('2011', 'X')`, ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), `('2013', 'X')`, ('.', '.')\n",
    "<hr>\n",
    "2. ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), `('domineering', 'X')`, ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), `('personality', 'X')`, ('.', '.')\n",
    "<hr>\n",
    "3. ('The', 'DET'), `('2018', 'X')`, `('FIFA', 'X')`, ('World', 'NOUN'), `('Cup', 'X')`, ('is', 'VERB'), ('the', 'DET'), `('21st', 'X')`, `('FIFA', 'X')`, ('World', 'NOUN'), `('Cup', 'X')`, (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), `('tournament', 'X')`, `('contested', 'X')`, ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NOUN'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NOUN'), ('.', '.'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NOUN'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'NOUN'), (\"'s\", 'PRT'), ('firehose', 'NOUN'), ('.', '.'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'NOUN'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'NOUN'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'NOUN'), ('.', '.'), ('The', 'DET'), ('2018', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'NOUN'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'NOUN'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'NOUN'), ('invited', 'NOUN'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n",
      "Wall time: 40.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tagging the test sentences\n",
    "test_tagged_seq = Modified_Viterbi_1(test_set)\n",
    "print(test_tagged_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Following are the highlighted tags which got corrected using this methods:***\n",
    "1. `('Android', 'NOUN')`, ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), `('Google', 'NOUN')`, ('.', '.'), `('Android', 'NOUN')`, ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), `('OS', 'NOUN')`, `('worldwide', 'NOUN')`, ('on', 'ADP'), `('smartphones', 'NOUN')`, ('since', 'ADP'), ('2011', 'NOUN'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NOUN'), ('.', '.')\n",
    "<hr>\n",
    "2. ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'NOUN'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), `('personality', 'NOUN')`, ('.', '.')\n",
    "<hr>\n",
    "3. ('The', 'DET'), ('2018', 'NOUN'), `('FIFA', 'NOUN')`, ('World', 'NOUN'), `('Cup', 'NOUN')`, ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), `('FIFA', 'NOUN')`, ('World', 'NOUN'), `('Cup', 'NOUN')`, (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), `('tournament', 'NOUN')`, ('contested', 'NOUN'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'VERB'), ('since', 'ADP'), ('2011', 'NUM'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NUM'), ('.', '.'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NUM'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'NOUN'), (\"'s\", 'PRT'), ('firehose', 'NOUN'), ('.', '.'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'VERB'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'VERB'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'NOUN'), ('.', '.'), ('The', 'DET'), ('2018', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'VERB'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'VERB'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'NOUN'), ('invited', 'VERB'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tagging the test sentences\n",
    "test_tagged_seq = Modified_Viterbi_2(test_set)\n",
    "print(test_tagged_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Following are the highlighted tags which got corrected using this methods:***\n",
    "1. `('Android', 'NOUN')`, ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), `('Google', 'NOUN')`, ('.', '.'), `('Android', 'NOUN')`, ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), `('OS', 'NOUN')`, `('worldwide', 'NOUN')`, ('on', 'ADP'), `('smartphones', 'NOUN')`, ('since', 'ADP'), `('2011', 'NUM')`, ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), `('2013', 'NUM')`, ('.', '.')\n",
    "<hr>\n",
    "2. ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), `('domineering', 'VERB')`, ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), `('personality', 'NOUN')`, ('.', '.')\n",
    "<hr>\n",
    "3. ('The', 'DET'), `('2018', 'NUM')`, `('FIFA', 'NOUN')`, ('World', 'NOUN'), `('Cup', 'NOUN')`, ('is', 'VERB'), ('the', 'DET'), `('21st', 'NUM')`, `('FIFA', 'NOUN')`, ('World', 'NOUN'), `('Cup', 'NOUN')`, (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), `('tournament', 'NOUN')`, `('contested', 'VERB')`, ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
